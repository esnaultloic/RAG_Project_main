üöÄ RAG-CV-Assistant : Interrogation de CV (LlamaIndex + Gemini)Ce projet est un syst√®me de Generation Augment√©e par la R√©cup√©ration (RAG) con√ßu pour r√©pondre √† des questions sur un document (CV au format PDF ou autre) en utilisant l'Intelligence Artificielle.Il utilise LlamaIndex pour l'ingestion de donn√©es et la structuration du RAG, ChromaDB pour la persistance vectorielle, et les mod√®les Google Gemini pour l'embedding et la g√©n√©ration de la r√©ponse finale.üåü ObjectifL'objectif principal est de cr√©er un agent intelligent capable de lire, comprendre, et synth√©tiser les informations d'un CV (ou tout autre document) pour r√©pondre √† des requ√™tes pr√©cises, en s'assurant que la r√©ponse soit toujours ancr√©e dans le contenu du document fourni.üèóÔ∏è Architecture et TechnologiesComposantTechnologieR√¥leFramework RAGllama_indexG√®re le pipeline complet : chargement, indexation, et requ√™te.LLM (G√©n√©ration)GoogleGenAI (Gemini 2.5 Flash)Fournit la r√©ponse finale, bas√©e sur les chunks r√©cup√©r√©s.Mod√®le d'EmbeddingGoogleGenAIEmbedding (text-embedding-004)Transforme le texte en vecteurs pour la recherche s√©mantique.Base de Donn√©es VectorielleChromaDB (PersistentClient)Stocke l'index vectoriel de mani√®re persistante sur disque (./chroma_db).Configurationpython-dotenvCharge la cl√© d'API (GEMINI_API_KEY) depuis un fichier .env.üõ†Ô∏è Configuration et D√©marragePr√©-requis : Python 3.9+Installation des d√©pendances :Bashpip install llama-index llama-index-llms-google-genai llama-index-embeddings-google-genai llama-index-vector-stores-chroma chromadb python-dotenv
Cl√© d'API :Cr√©ez un fichier nomm√© .env √† la racine du projet et ins√©rez votre cl√© d'API Google Gemini :GEMINI_API_KEY="VOTRE_CLE_API_ICI"
Dossier des donn√©es :Cr√©ez un dossier nomm√© data/ et placez votre CV (e.g., mon_cv.pdf) √† l'int√©rieur.Ex√©cution :Bashpython your_script_name.py
üß† √âtat Actuel du Code (your_script_name.py)Le script est un pipeline RAG complet et fonctionnel, structur√© autour de deux fonctions principales :1. setup_rag_index(data_dir: str = "data")R√¥le : Initialise la base de donn√©es ChromaDB (./chroma_db) et l'index RAG.Logique de Persistance :Le code v√©rifie d'abord si la collection cv_rag_collection existe d√©j√† dans ChromaDB.Si elle existe (Loading existing collection), l'index est charg√© sans re-traiter le document.Si elle n'existe pas (Creating and indexing new collection), les documents dans data/ sont lus, l'index est cr√©√©, et les vecteurs sont sauvegard√©s dans ChromaDB.2. query_rag(prompt: str) -> strR√¥le : Effectue une recherche vectorielle dans l'index (r√©cup√©ration) puis g√©n√®re une r√©ponse (g√©n√©ration) √† l'aide de Gemini, en s'appuyant uniquement sur les morceaux de texte pertinents du CV (RAG).Configuration : Utilise similarity_top_k=3 pour r√©cup√©rer les 3 meilleurs chunks de contexte.‚ö†Ô∏è Probl√®me Actuel et R√©sultat (Pour Cursor)Lors du test d'ex√©cution, le pipeline se connecte correctement √† la base de donn√©es existante mais retourne une r√©ponse vide (Empty Response) lors de la requ√™te de test.R√©sultat de l'ex√©cution :Loading existing collection: cv_rag_collection

[Test Query]: Quel est mon dernier poste et quelles √©taient mes principales responsabilit√©s ?

==================================================
[RAG Response]:
Empty Response
==================================================
Pistes de D√©bogage / Prochaines √âtapes :V√©rification de l'Index : S'assurer que les documents ont √©t√© correctement charg√©s et que des vecteurs existent dans la collection ChromaDB. Le probl√®me pourrait venir d'un index corrompu ou vide.Affichage de la R√©cup√©ration : Modifier temporairement query_rag pour afficher les chunks de contexte r√©cup√©r√©s par LlamaIndex avant la g√©n√©ration LLM. Cela permet de confirmer si la premi√®re √©tape (Retrieval) fonctionne.Taille des Chunks : Tester l'indexation avec une configuration de chunk_size et chunk_overlap personnalis√©e dans Settings.node_parser pour am√©liorer la pertinence des morceaux de texte r√©cup√©r√©s.